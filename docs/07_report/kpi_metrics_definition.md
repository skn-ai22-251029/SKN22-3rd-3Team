# ğŸ“Š ZIPSA KPI & Evaluation Metrics

í”„ë¡œì íŠ¸ì˜ ì„±ê³¼ ë° AI ëª¨ë¸ì˜ í’ˆì§ˆì„ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •í•˜ê¸° ìœ„í•œ í•µì‹¬ ì§€í‘œ(KPI) ì •ì˜ì„œì…ë‹ˆë‹¤.
LangSmith ë“±ì˜ ë„êµ¬ë¥¼ í†µí•´ ìë™ ì¸¡ì •ë˜ë©°, ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ ëŒ€ìƒì…ë‹ˆë‹¤.

---

## 1. Performance Metrics (ì„±ëŠ¥ ì§€í‘œ)
ì‹œìŠ¤í…œì˜ ì‘ë‹µì„±ê³¼ ì•ˆì •ì„±ì„ ì¸¡ì •í•©ë‹ˆë‹¤.

| Metric | Definition | Target Goal | Measurement |
| :--- | :--- | :--- | :--- |
| **E2E Latency** | ì‚¬ìš©ì ì§ˆë¬¸ í›„ ì²« ë²ˆì§¸ í† í°ì´ ìƒì„±ë˜ê¸°ê¹Œì§€ ê±¸ë¦¬ëŠ” ì‹œê°„ (TTFT) ë° ì „ì²´ ì™„ë£Œ ì‹œê°„. | TTFT < 1.5s<br>Total < 5s | Streamlit `st.session_state` íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë¡ |
| **Error Rate** | ì „ì²´ ìš”ì²­ ì¤‘ ì˜ˆì™¸(Exception)ë‚˜ íƒ€ì„ì•„ì›ƒì´ ë°œìƒí•œ ë¹„ìœ¨. | < 1% | Log Error Count / Total Requests |
| **Routing Accuracy** | ì‚¬ìš©ìì˜ ì˜ë„(Intent)ì— ë§ê²Œ ì˜¬ë°”ë¥¸ ì—ì´ì „íŠ¸(`Liaison`/`Matchmaker`)ë¡œ ë¼ìš°íŒ…ëœ ë¹„ìœ¨. | > 95% | LangSmith Tracer (Actual vs Expected) |

---

## 2. Cost & Efficiency Metrics (ë¹„ìš©/íš¨ìœ¨)
ìš´ì˜ ë¹„ìš©ì„ ìµœì í™”í•˜ê¸° ìœ„í•œ ì§€í‘œì…ë‹ˆë‹¤.

| Metric | Definition | Target Goal | Measurement |
| :--- | :--- | :--- | :--- |
| **Token Usage** | ìš”ì²­(Turn) ë‹¹ ì†Œë¹„ë˜ëŠ” í‰ê·  Input/Output í† í° ìˆ˜. | Avg < 1k tokens | OpenAI Usage API Response |
| **Cost per Turn** | ëŒ€í™” í•œ í„´ë‹¹ ë°œìƒí•˜ëŠ” ë¹„ìš© (USD). | < $0.01 | Token Count * Model Unit Price |
| **Cache Hit Rate** | ë™ì¼/ìœ ì‚¬ ì§ˆë¬¸ì— ëŒ€í•´ Redis ìºì‹œê°€ ì‘ë‹µí•œ ë¹„ìœ¨. | > 30% | Redis Hits / Total Queries |

---

## 3. Quality Validation Metrics (í’ˆì§ˆ ì§€í‘œ)
LLMì˜ ë‹µë³€ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤. (LLM-as-a-Judge í™œìš©)

| Metric | Definition | Evaluation Method |
| :--- | :--- | :--- |
| **Faithfulness** | ë‹µë³€ì´ ê²€ìƒ‰ëœ ë¬¸ì„œ(Context/RAG)ì˜ ë‚´ìš©ì— **ì¶©ì‹¤í•œê°€?** (í™˜ê° ì—¬ë¶€) | Ragas / LangSmith Evaluator |
| **Answer Relevance** | ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸(Query)ì— **ì ì ˆí•˜ê²Œ ëŒ€ë‹µí–ˆëŠ”ê°€?** | Ragas / LangSmith Evaluator |
| **Context Precsion** | ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì´ ì§ˆë¬¸ê³¼ **ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ë†’ì€ê°€?** (Retriever ì„±ëŠ¥) | Ragas / LangSmith Evaluator |

---

## 4. User Experience Metrics (ì‚¬ìš©ì ê²½í—˜)
ì‹¤ì œ ì‚¬ìš©ìì˜ ë§Œì¡±ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.

| Metric | Definition | Measurement |
| :--- | :--- | :--- |
| **Feedback Score** | ë‹µë³€ì— ëŒ€í•œ ì¢‹ì•„ìš”(ğŸ‘)/ì‹«ì–´ìš”(ğŸ‘) ë¹„ìœ¨. | UI Feedback Widget Data |
| **Session Length** | ì‚¬ìš©ìê°€ ì´íƒˆí•˜ì§€ ì•Šê³  ëŒ€í™”ë¥¼ ì´ì–´ê°€ëŠ” í‰ê·  í„´ ìˆ˜. | Avg Turns per SessionID |

---

## 5. Implementation Strategy (êµ¬í˜„ ì‹œë‚˜ë¦¬ì˜¤)

ì§€í‘œë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ ë³„ë„ì˜ ëŒ€ì‹œë³´ë“œë¥¼ ì§ì ‘ ê°œë°œí•˜ëŠ” ê²ƒì€ **ë¹„íš¨ìœ¨ì (Over-engineering)** ì…ë‹ˆë‹¤.
ì´ˆê¸°ì—ëŠ” **LangSmith**ì˜ ë‚´ì¥ ê¸°ëŠ¥ì„ 100% í™œìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

### 5.1 Feedback Collection Flow (ì¢‹ì•„ìš”/ì‹«ì–´ìš”)
1.  **UI (Next.js/Streamlit)**: ë‹µë³€ í•˜ë‹¨ì— ğŸ‘/ğŸ‘ ë²„íŠ¼ ë°°ì¹˜.
2.  **Action**: ì‚¬ìš©ìê°€ ë²„íŠ¼ í´ë¦­ ì‹œ, í•´ë‹¹ ë‹µë³€ì˜ `run_id`ì™€ `score` (1 or 0)ë¥¼ APIë¡œ ì „ì†¡.
3.  **Backend**: LangSmith Clientë¥¼ í†µí•´ í”¼ë“œë°± ë“±ë¡.
    ```python
    langsmith.client.create_feedback(
        run_id=answer_run_id,
        key="user_score",
        score=1.0  # or 0.0
    )
    ```
4.  **Dashboard**: LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ ìë™ìœ¼ë¡œ User Score í†µê³„ ì‹œê°í™”ë¨. (ì§ì ‘ ê°œë°œ X)

### 5.2 Session Metrics (í„´ ìˆ˜ ì¸¡ì •)
1.  **Trace Grouping**: ëª¨ë“  ì—ì´ì „íŠ¸ í˜¸ì¶œ ì‹œ `session_id`ë¥¼ LangSmith Tracerì— íƒœê¹….
2.  **Analytics**: LangSmithê°€ `session_id` ê¸°ì¤€ìœ¼ë¡œ ìë™ìœ¼ë¡œ ê·¸ë£¹í•‘í•˜ì—¬ **í‰ê·  ì„¸ì…˜ ê¸¸ì´(Token/Wards)** ë¥¼ ê³„ì‚°í•´ì¤Œ.

### 5.3 Custom Dashboard í•„ìš”ì„±?
*   **Phase 1~3**: **ë¶ˆí•„ìš” (Not Essential)**. LangSmith ê¸°ë³¸ ëŒ€ì‹œë³´ë“œë¡œ ì¶©ë¶„í•¨.
*   **Phase 4**: ë¹„ì¦ˆë‹ˆìŠ¤ íŒ€ì´ SaaS í˜•íƒœë¡œ ë³´ê³  ì‹¶ì–´í•  ë•Œ, ê·¸ë•Œ ê°€ì„œ Retoolì´ë‚˜ Grafanaë¡œ ì—°ë™ ê³ ë ¤.
