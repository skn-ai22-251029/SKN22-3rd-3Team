{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§‘â€âš–ï¸ LLM-as-a-judge: Advanced Retrieval Evaluation\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­(Hit@k)ì„ ë„˜ì–´, **LLM(GPT-4o)**ì„ ì‚¬ìš©í•˜ì—¬ ë¦¬íŠ¸ë¦¬ë²„ì˜ ì‹¤ì œ ì„±ëŠ¥ì„ ì •ì„±ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ğŸš€ ì£¼ìš” í”„ë¡œì„¸ìŠ¤\n",
    "1.  **Hard Query Augmentation**: ê¸°ì¡´ì˜ ë‹¨ìˆœí•œ ì§ˆë¬¸ì„ ì‹¤ì œ ìœ ì €ë“¤ì´ í•  ë²•í•œ ëª¨í˜¸í•˜ê³  ë³µì¡í•œ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "2.  **Hybrid Retrieval**: ë³€í™˜ëœ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "3.  **LLM Scoring**: ê²€ìƒ‰ ê²°ê³¼ê°€ ì§ˆë¬¸ì˜ ì˜ë„ì— ì–¼ë§ˆë‚˜ ë¶€í•©í•˜ëŠ”ì§€ 1~5ì  ì²™ë„ë¡œ ì±„ì í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ì‹¤í—˜ íŒŒë¼ë¯¸í„°\n",
    "SAMPLE_SIZE = 10        # ë¹„ìš© ë¬¸ì œë¡œ ì ì€ ìƒ˜í”Œë¡œ ì‹œì‘ ê¶Œì¥\n",
    "JUDGE_MODEL = \"gpt-4o\"  # í‰ê°€ì ëª¨ë¸\n",
    "RETRIEVER_K = 5         # ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜\n",
    "\n",
    "# í™˜ê²½ ì„¤ì •\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import asyncio\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LangChain\n",
    "from langchain.chat_models import init_chat_model\n",
    "from src.core.config import LLMConfig\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "load_dotenv(os.path.join(project_root, \".env\"))\n",
    "print(f\"Project Root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Components\n",
    "from src.retrieval.hybrid_search import HybridRetriever\n",
    "\n",
    "retriever = HybridRetriever(version=\"v3\", collection_name=\"care_guides\")\n",
    "llm_judge = init_chat_model(JUDGE_MODEL, model_provider=\"openai\", temperature=0)\n",
    "\n",
    "print(\"âœ… Components Initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë°ì´í„°ì…‹ ë¡œë“œ ë° ìƒ˜í”Œë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(project_root, \"data/v3/golden_dataset.json\")\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        FULL_DATASET = json.load(f)\n",
    "    print(f\"ğŸ“š Loaded {len(FULL_DATASET)} test cases.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Dataset not found. Using mock.\")\n",
    "    FULL_DATASET = [{\"query\": \"ë©”ì¸ì¿¤ íŠ¹ì§•\", \"expected_keyword\": \"ë©”ì¸ ì¿¤\"}]\n",
    "\n",
    "test_dataset = random.sample(FULL_DATASET, min(SAMPLE_SIZE, len(FULL_DATASET)))\n",
    "print(f\"ğŸ§ª Selected {len(test_dataset)} samples for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hard Query Generation (Augmentation)\n",
    "ë‹¨ìˆœí•œ í‚¤ì›Œë“œí˜• ì§ˆë¬¸ì„ ë¬¸ë§¥ì´ í¬í•¨ëœ ë³µí•© ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "- **Before**: \"ë©”ì¸ì¿¤ íŠ¹ì§•\"\n",
    "- **After**: \"ë©ì¹˜ê°€ í¬ê³  í„¸ì´ í’ì„±í•œ ê³ ì–‘ì´ë¥¼ í‚¤ìš°ê³  ì‹¶ì€ë°, ì„±ê²©ì´ ê°•ì•„ì§€ì²˜ëŸ¼ ì˜¨ìˆœí•œ í’ˆì¢…ì´ ë­ê°€ ìˆì„ê¹Œ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedQuery(BaseModel):\n",
    "    hard_query: str = Field(..., description=\"The transformed complex query\")\n",
    "\n",
    "augment_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a creative user helper. Convert the given simple query into a more natural, complex, and slightly ambiguous user question that maintains the original intent but is harder for a keyword search engine to match. Do NOT include the specific target keyword if possible, describe its characteristics instead.\"),\n",
    "    (\"human\", \"Original Query: {query}\\nTarget Keyword: {target}\")\n",
    "])\n",
    "\n",
    "augment_chain = augment_prompt | llm_judge.with_structured_output(AugmentedQuery)\n",
    "\n",
    "async def generate_hard_queries(dataset):\n",
    "    augmented_dataset = []\n",
    "    print(\"ğŸš€ Generating Hard Queries...\")\n",
    "    \n",
    "    for item in dataset:\n",
    "        original_query = item.get(\"query\")\n",
    "        target = item.get(\"expected_keyword\", item.get(\"target\"))\n",
    "        \n",
    "        try:\n",
    "            res = await augment_chain.ainvoke({\"query\": original_query, \"target\": target})\n",
    "            augmented_dataset.append({\n",
    "                \"original_query\": original_query,\n",
    "                \"hard_query\": res.hard_query,\n",
    "                \"target\": target\n",
    "            })\n",
    "            print(f\" - {original_query} -> {res.hard_query}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error generating query for {original_query}: {e}\")\n",
    "            \n",
    "    return augmented_dataset\n",
    "\n",
    "# ì‹¤í–‰\n",
    "augmented_data = await generate_hard_queries(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieval & LLM Evaluation\n",
    "ìƒì„±ëœ Hard Queryë¡œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ , ê²°ê³¼ë¥¼ LLMì´ ì±„ì í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‰ê°€ ê²°ê³¼ ëª¨ë¸\n",
    "class JudgeResult(BaseModel):\n",
    "    score: int = Field(..., description=\"Score from 1 to 5\")\n",
    "    reasoning: str = Field(..., description=\"Reasoning for the score\")\n",
    "\n",
    "# í‰ê°€ í”„ë¡¬í”„íŠ¸\n",
    "judge_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert search relevance evaluator.\n",
    "Apply the following score rubric to the retrieved documents for the user query:\n",
    "\n",
    "Score 5: The document provides a perfect answer to the user's implicit and explicit intent. It contains the core semantic meaning or the specific target entity described.\n",
    "Score 4: The document is very relevant and helpful, but might contain slight noise or misses a minor detail.\n",
    "Score 3: The document is somewhat relevant (mentions related concepts), but does not directly answer the core intent or is too broad.\n",
    "Score 2: The document has very low relevance (perhaps shares a keyword) but does not help the user.\n",
    "Score 1: The document is completely irrelevant.\n",
    "\n",
    "User Query: {query}\n",
    "Expected Topic/Target: {target}\n",
    "\n",
    "Retrieved Documents:\n",
    "{context}\n",
    "\"\"\"),\n",
    "])\n",
    "\n",
    "judge_chain = judge_prompt | llm_judge.with_structured_output(JudgeResult)\n",
    "\n",
    "# ==================================================================================\n",
    "# ğŸ”„ RAG Context Distillation (Same with Agent Logic)\n",
    "# ----------------------------------------------------------------------------------\n",
    "# ì‹¤ì œ ì—ì´ì „íŠ¸(Care Team, Liaison)ì™€ ì™„ì „íˆ ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ ë¬¸ë§¥ì„ ì••ì¶•í•©ë‹ˆë‹¤.\n",
    "# ==================================================================================\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "llm_distill = init_chat_model(LLMConfig.ROUTER_MODEL, model_provider=\"openai\", temperature=0)\n",
    "\n",
    "async def distill_context(query, docs):\n",
    "    if not docs:\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. ë¬¸ì„œ ë¸”ë¡ êµ¬ì„± (Agentì™€ ë™ì¼ í¬ë§·)\n",
    "    docs_block = \"\\n\\n\".join([\n",
    "        f\"[{r.get('title_refined', '') or r.get('title', '')}]\\n{r.get('text', '')[:1500]}\"\n",
    "        for r in docs\n",
    "    ])\n",
    "    \n",
    "    # 2. LLM í˜¸ì¶œ (Agentì™€ ë™ì¼ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° - ë‹¨ì¼ SystemMessage)\n",
    "    messages = [\n",
    "        SystemMessage(content=(\n",
    "            \"ì•„ë˜ ì°¸ê³  ë¬¸ì„œë“¤ì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° í•„ìš”í•œ í•µì‹¬ ì •ë³´ë§Œ ê°„ê²°í•˜ê²Œ ì¶”ì¶œí•˜ì„¸ìš”.\\n\"\n",
    "            \"- ë¶ˆí•„ìš”í•œ ì„œë¡ /ë°˜ë³µ ì œê±°, í•µì‹¬ íŒ©íŠ¸ì™€ ìˆ˜ì¹˜ ìœ„ì£¼ë¡œ ì •ë¦¬\\n\"\n",
    "            \"- 3~5ê°œ bullet point, ì´ 300ì ì´ë‚´\\n\\n\"\n",
    "            f\"[ì‚¬ìš©ì ì§ˆë¬¸]\\n{query}\\n\\n\"\n",
    "            f\"[ì°¸ê³  ë¬¸ì„œ]\\n{docs_block}\"\n",
    "        ))\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        res = await llm_distill.ainvoke(messages)\n",
    "        return res.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error distilling context: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "async def run_llm_judge(aug_data):\n",
    "    results = []\n",
    "    print(\"\\nâš–ï¸ Starting LLM Evaluation...\")\n",
    "    \n",
    "    for item in aug_data:\n",
    "        query = item['hard_query']\n",
    "        target = item['target']\n",
    "        \n",
    "        # 1. ê²€ìƒ‰\n",
    "        # SpecialistëŠ” Noneìœ¼ë¡œ ê°€ì • (Hard Queryë¼ì„œ ë¶„ë¥˜ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ)\n",
    "        docs = await retriever.search(query, limit=RETRIEVER_K)\n",
    "        \n",
    "        # 2. Context êµ¬ì„± (Distillation ì ìš©)\n",
    "        context_text = await distill_context(query, docs)\n",
    "        \n",
    "        # 3. í‰ê°€\n",
    "        try:\n",
    "            eval_res = await judge_chain.ainvoke({\n",
    "                \"query\": query, \n",
    "                \"target\": target,\n",
    "                \"context\": context_text\n",
    "            })\n",
    "            \n",
    "            results.append({\n",
    "                \"Original\": item['original_query'],\n",
    "                \"Hard_Query\": query,\n",
    "                \"Score\": eval_res.score,\n",
    "                \"Reasoning\": eval_res.reasoning,\n",
    "                \"Top1_Doc\": docs[0].get('title', 'No Result') if docs else 'None'\n",
    "            })\n",
    "            print(f\"User: {query[:30]}... | Score: {eval_res.score}/5\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {query}: {e}\")\n",
    "            \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# í‰ê°€ ì‹¤í–‰\n",
    "df_eval = await run_llm_judge(augmented_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ë¶„ì„\n",
    "print(\"=\"*40)\n",
    "print(f\"ğŸ† Average Score: {df_eval['Score'].mean():.2f} / 5.0\")\n",
    "print(\"=\"*40)\n",
    "display(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skn-third-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
