{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Retriever Performance Comparison (N=5 Trials)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **ì„¸ ê°€ì§€ ë¦¬íŠ¸ë¦¬ë²„(BM25, Vector, Hybrid)**ì˜ ì„±ëŠ¥ì„ **5íšŒì˜ ë…ë¦½ì ì¸ ì‹œí–‰**ì„ í†µí•´ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ðŸ§ª ì‹¤í—˜ ì¡°ê±´\n",
    "- **Dataset**: `golden_dataset.json` (ì´ 3,000+ê°œ ì¤‘ ë¬´ìž‘ìœ„ ì¶”ì¶œ)\n",
    "- **Trials**: 5íšŒ ë°˜ë³µ (ê° ì‹œí–‰ë§ˆë‹¤ **ë™ì¼í•œ 50ê°œ ìƒ˜í”Œ**ë¡œ 3ê°œ ëª¨ë¸ ëª¨ë‘ í…ŒìŠ¤íŠ¸)\n",
    "- **Metrics**: \n",
    "  - **Hit@5**: ìƒìœ„ 5ê°œ ê²°ê³¼ ë‚´ ì •ë‹µ í¬í•¨ ì—¬ë¶€\n",
    "  - **MRR**: Mean Reciprocal Rank (ì •ë‹µ ìˆœìœ„ì˜ íš¨ìœ¨ì„±)\n",
    "- **Visualization**: Trialë³„ ì„±ëŠ¥ ë³€í™” ë° ì „ì²´ í‰ê·  ë¹„êµ ì°¨íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Parameters\n",
    "N_TRIALS = 5\n",
    "SAMPLE_SIZE_PER_TRIAL = 50\n",
    "K_VALUE = 5\n",
    "\n",
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import asyncio\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "load_dotenv(os.path.join(project_root, \".env\"))\n",
    "print(f\"Project Root: {project_root}\")\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (Mac OS) - ê·¸ëž˜í”„ ê¹¨ì§ ë°©ì§€\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Retrievers\n",
    "from src.retrieval.bm25_retriever import BM25Retriever\n",
    "from src.retrieval.vector_retriever import VectorRetriever\n",
    "from src.retrieval.hybrid_search import HybridRetriever\n",
    "\n",
    "print(\"ðŸš€ Initializing Retrievers...\")\n",
    "bm25 = BM25Retriever(version=\"v3\", collection_name=\"care_guides\")\n",
    "vector = VectorRetriever(version=\"v3\", collection_name=\"care_guides\")\n",
    "hybrid = HybridRetriever(version=\"v3\", collection_name=\"care_guides\")\n",
    "print(\"âœ… All Retrievers Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "dataset_path = os.path.join(project_root, \"data/v3/golden_dataset.json\")\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        FULL_DATASET = json.load(f)\n",
    "    print(f\"ðŸ“š Loaded {len(FULL_DATASET)} total cases.\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Dataset not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§ª Evaluation Function\n",
    "async def evaluate(retriever, dataset, name, k=5):\n",
    "    hits = 0\n",
    "    reciprocal_ranks = 0\n",
    "    \n",
    "    for item in dataset:\n",
    "        query = item.get(\"query\")\n",
    "        target = item.get(\"expected_keyword\", item.get(\"target\"))\n",
    "        \n",
    "        try:\n",
    "            specialist = item.get(\"specialist\")\n",
    "            results = await retriever.search(query, specialist=specialist, limit=k)\n",
    "        except Exception:\n",
    "            results = []\n",
    "            \n",
    "        found_rank = None\n",
    "        for rank, res in enumerate(results):\n",
    "            content = (\n",
    "                res.get('name_ko', '') + \" \" + \n",
    "                res.get('title_refined', '') + \" \" + \n",
    "                res.get('text', '')\n",
    "            ).lower()\n",
    "            \n",
    "            if target.lower() in content:\n",
    "                found_rank = rank + 1\n",
    "                break\n",
    "        \n",
    "        if found_rank:\n",
    "            hits += 1\n",
    "            reciprocal_ranks += 1.0 / found_rank\n",
    "            \n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Hit@5\": hits / len(dataset),\n",
    "        \"MRR\": reciprocal_ranks / len(dataset)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”„ Run Experiment Loop\n",
    "results_log = []\n",
    "\n",
    "print(f\"ðŸ Starting {N_TRIALS} Trials...\")\n",
    "\n",
    "for i in range(1, N_TRIALS + 1):\n",
    "    print(f\"\\n[Trial {i}/{N_TRIALS}] Sampling Data...\")\n",
    "    \n",
    "    # 1. Random Sampling (Same dataset for all models in this trial)\n",
    "    trial_data = random.sample(FULL_DATASET, min(SAMPLE_SIZE_PER_TRIAL, len(FULL_DATASET)))\n",
    "    \n",
    "    # 2. Evaluate Models\n",
    "    res_bm25 = await evaluate(bm25, trial_data, \"BM25\", k=K_VALUE)\n",
    "    res_vector = await evaluate(vector, trial_data, \"Vector\", k=K_VALUE)\n",
    "    res_hybrid = await evaluate(hybrid, trial_data, \"Hybrid\", k=K_VALUE)\n",
    "    \n",
    "    # 3. Log Results\n",
    "    for res in [res_bm25, res_vector, res_hybrid]:\n",
    "        res[\"Trial\"] = i\n",
    "        results_log.append(res)\n",
    "        print(f\"  > {res['Model']}: Hit@5={res['Hit@5']:.2f}, MRR={res['MRR']:.2f}\")\n",
    "        \n",
    "df_results = pd.DataFrame(results_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated Stats\n",
    "summary = df_results.groupby(\"Model\")[[\"Hit@5\", \"MRR\"]].agg([\"mean\", \"std\"])\n",
    "display(summary)\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# 1. Hit@5 Comparison\n",
    "sns.barplot(data=df_results, x=\"Model\", y=\"Hit@5\", ax=axes[0], palette=\"viridis\", errorbar=\"sd\")\n",
    "axes[0].set_title(\"Hit@5 Score Comparison (Mean Â± SD)\")\n",
    "axes[0].set_ylim(0, 1.0)\n",
    "for p in axes[0].patches:\n",
    "    axes[0].annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                     ha = 'center', va = 'bottom', xytext = (0, 5), textcoords = 'offset points')\n",
    "\n",
    "# 2. MRR Comparison\n",
    "sns.barplot(data=df_results, x=\"Model\", y=\"MRR\", ax=axes[1], palette=\"magma\", errorbar=\"sd\")\n",
    "axes[1].set_title(\"MRR Score Comparison (Mean Â± SD)\")\n",
    "axes[1].set_ylim(0, 1.0)\n",
    "for p in axes[1].patches:\n",
    "    axes[1].annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                     ha = 'center', va = 'bottom', xytext = (0, 5), textcoords = 'offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial-wise Line Plot (Consistency Check)\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.lineplot(data=df_results, x=\"Trial\", y=\"MRR\", hue=\"Model\", marker=\"o\", ax=ax)\n",
    "ax.set_title(\"MRR Stability Across 5 Trials\")\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.set_xticks(range(1, N_TRIALS + 1))\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skn-third-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
