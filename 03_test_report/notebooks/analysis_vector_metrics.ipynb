{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Retriever ì„±ëŠ¥ ë¶„ì„ (Hit@k & MRR)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **VectorRetriever**ì˜ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ì„±ëŠ¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ì‹¤í—˜ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "SAMPLE_SIZE = 50\n",
    "K_VALUES = [1, 3, 5]\n",
    "\n",
    "# í™˜ê²½ ì„¤ì •\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "load_dotenv(os.path.join(project_root, \".env\"))\n",
    "print(f\"Project Root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retrieval.vector_retriever import VectorRetriever\n",
    "\n",
    "retriever = VectorRetriever(version=\"v3\", collection_name=\"care_guides\")\n",
    "print(\"âœ… VectorRetriever initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë°ì´í„°ì…‹ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(project_root, \"data/v3/golden_dataset.json\")\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        FULL_DATASET = json.load(f)\n",
    "    print(f\"ğŸ“š Loaded {len(FULL_DATASET)} test cases.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Dataset not found. Using mock.\")\n",
    "    FULL_DATASET = [{\"query\": \"ë©”ì¸ì¿¤ íŠ¹ì§•\", \"expected_keyword\": \"ë©”ì¸ ì¿¤\"}]\n",
    "\n",
    "test_dataset = random.sample(FULL_DATASET, min(SAMPLE_SIZE, len(FULL_DATASET)))\n",
    "print(f\"ğŸ§ª Selected {len(test_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í‰ê°€ ë¡œì§ êµ¬í˜„ (Hit@k, MRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_vector(dataset, k_values):\n",
    "    results_summary = {k: 0 for k in k_values}\n",
    "    total_mrr = 0\n",
    "    log_data = []\n",
    "    \n",
    "    max_k = max(k_values)\n",
    "    \n",
    "    for item in dataset:\n",
    "        query = item.get(\"query\")\n",
    "        target = item.get(\"expected_keyword\", item.get(\"target\"))\n",
    "        \n",
    "        if not target: continue\n",
    "        \n",
    "        specialist = item.get(\"specialist\")\n",
    "        search_results = await retriever.search(query, specialist=specialist, limit=max_k)\n",
    "        \n",
    "        found_rank = None\n",
    "        for rank, res in enumerate(search_results):\n",
    "            content = (\n",
    "                res.get('name_ko', '') + \" \" + \n",
    "                res.get('title_refined', '') + \" \" + \n",
    "                res.get('text', '')\n",
    "            ).lower()\n",
    "            \n",
    "            if target.lower() in content:\n",
    "                found_rank = rank + 1\n",
    "                break\n",
    "        \n",
    "        reciprocal_rank = 0\n",
    "        hit_status = \"âŒ\"\n",
    "        \n",
    "        if found_rank:\n",
    "            reciprocal_rank = 1 / found_rank\n",
    "            hit_status = f\"âœ… (Rank {found_rank})\"\n",
    "            \n",
    "            for k in k_values:\n",
    "                if found_rank <= k:\n",
    "                    results_summary[k] += 1\n",
    "        \n",
    "        total_mrr += reciprocal_rank\n",
    "        \n",
    "        log_data.append({\n",
    "            \"Query\": query,\n",
    "            \"Target\": target,\n",
    "            \"Result\": hit_status,\n",
    "            \"MRR\": round(reciprocal_rank, 4)\n",
    "        })\n",
    "        \n",
    "    # ìµœì¢… ì§‘ê³„\n",
    "    total_count = len(dataset)\n",
    "    metrics = {\n",
    "        f\"Hit@{k}\": round(count / total_count, 4) \n",
    "        for k, count in results_summary.items()\n",
    "    }\n",
    "    metrics[\"MRR\"] = round(total_mrr / total_count, 4)\n",
    "    \n",
    "    return metrics, pd.DataFrame(log_data)\n",
    "\n",
    "print(\"Evaluator ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, df_log = await evaluate_vector(test_dataset, K_VALUES)\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(\"   ğŸ“Š Vector Evaluation Results\")\n",
    "print(\"=\"*40)\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k:<10}: {v}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "display(df_log.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skn-third-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
