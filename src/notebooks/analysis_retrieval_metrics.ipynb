{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever 성능 평가 및 RRF 분석 (Hit@k)\n",
    "\n",
    "이 노트북은 BM25, Vector, Hybrid(RRF) 검색의 성능을 정량적으로 평가(Hit@k)하고 비교 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 설정\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# 프로젝트 루트 경로 설정\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "load_dotenv(os.path.join(project_root, \".env\"))\n",
    "print(f\"Project Root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retrieval.bm25_retriever import BM25Retriever\n",
    "from src.retrieval.vector_retriever import VectorRetriever\n",
    "from src.retrieval.hybrid_search import HybridRetriever\n",
    "\n",
    "bm25 = BM25Retriever(version=\"v3\", collection_name=\"care_guides\")\n",
    "vector = VectorRetriever(version=\"v3\", collection_name=\"care_guides\")\n",
    "hybrid = HybridRetriever(version=\"v3\", collection_name=\"care_guides\")\n",
    "\n",
    "print(\"✅ All Retrievers initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ground Truth 데이터셋 정의\n",
    "평가를 위한 [질문, 정답(품종명)] 쌍을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = [\n",
    "    {\"query\": \"메인쿤 특징\", \"target\": \"메인 쿤\"},\n",
    "    {\"query\": \"렉돌 성격\", \"target\": \"랙돌\"},\n",
    "    {\"query\": \"페르시안 고양이\", \"target\": \"페르시안\"},\n",
    "    {\"query\": \"털 안빠지는 고양이\", \"target\": \"스핑크스\"}, # 예시: 스핑크스가 나오길 기대\n",
    "    {\"query\": \"다리 짧은 고양이\", \"target\": \"먼치킨\"},\n",
    "    {\"query\": \"귀 접힌 고양이\", \"target\": \"스코티시 폴드\"}\n",
    "]\n",
    "\n",
    "print(f\"Dataset size: {len(ground_truth)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hit@k 평가 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def calculate_hit_at_k(retriever_func, k_list=[1, 3, 5]):\n",
    "    results_summary = {k: 0 for k in k_list}\n",
    "    \n",
    "    for item in ground_truth:\n",
    "        query = item[\"query\"]\n",
    "        target = item[\"target\"]\n",
    "        \n",
    "        # 검색 실행 (최대 k_list의 max값만큼)\n",
    "        max_k = max(k_list)\n",
    "        search_results = await retriever_func(query, limit=max_k)\n",
    "        \n",
    "        # 검색 결과 중 target이 포함되어 있는지 확인\n",
    "        found_ranks = []\n",
    "        for rank, res in enumerate(search_results):\n",
    "            if target in res.get('name_ko', '') or target in res.get('name_en', ''):\n",
    "                found_ranks.append(rank + 1)\n",
    "        \n",
    "        # Hit@k 계산\n",
    "        for k in k_list:\n",
    "            if any(rank <= k for rank in found_ranks):\n",
    "                results_summary[k] += 1\n",
    "                \n",
    "    # 평균 Hit@k\n",
    "    total = len(ground_truth)\n",
    "    return {f\"Hit@{k}\": round(count / total, 2) for k, count in results_summary.items()}\n",
    "\n",
    "print(\"Evaluator ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retriever별 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# 1. BM25\n",
    "print(\"Running BM25 evaluation...\")\n",
    "bm25_metrics = await calculate_hit_at_k(lambda q, limit: bm25.search(q, limit=limit))\n",
    "bm25_metrics[\"Retriever\"] = \"BM25\"\n",
    "results.append(bm25_metrics)\n",
    "\n",
    "# 2. Vector\n",
    "print(\"Running Vector evaluation...\")\n",
    "vector_metrics = await calculate_hit_at_k(lambda q, limit: vector.search(q, limit=limit))\n",
    "vector_metrics[\"Retriever\"] = \"Vector\"\n",
    "results.append(vector_metrics)\n",
    "\n",
    "# 3. Hybrid (RRF)\n",
    "print(\"Running Hybrid evaluation...\")\n",
    "hybrid_metrics = await calculate_hit_at_k(lambda q, limit: hybrid.search(q, limit=limit))\n",
    "hybrid_metrics[\"Retriever\"] = \"Hybrid (RRF)\"\n",
    "results.append(hybrid_metrics)\n",
    "\n",
    "# 결과 출력\n",
    "df = pd.DataFrame(results).set_index(\"Retriever\")\n",
    "df = df[[\"Hit@1\", \"Hit@3\", \"Hit@5\"]]\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
