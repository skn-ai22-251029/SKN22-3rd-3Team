{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(Hybrid Retrieval) ì‹¤í—˜ì‹¤\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ **HybridRetriever**ì˜ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ê³ , ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ íŠœë‹í•˜ê¸° ìœ„í•œ ê³µê°„ì…ë‹ˆë‹¤.\n",
        "ë²¡í„° ê²€ìƒ‰(Vector Search)ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰(Keyword Search)ì´ ê²°í•©ëœ RRF(Reciprocal Rank Fusion) ë°©ì‹ì˜ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.\n",
        "\n",
        "### ğŸ†• ì£¼ìš” ê¸°ëŠ¥\n",
        "- **ë‹¨ì¼ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸**: íŠ¹ì • ì§ˆë¬¸ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ ë° ì ìˆ˜ í™•ì¸\n",
        "- **ì„±ëŠ¥ í‰ê°€ (Evaluation)**: Hit@K, MRR ë“± ì •ëŸ‰ì  ì§€í‘œ ì¸¡ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# 1. í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •\n",
        "current_path = os.getcwd()\n",
        "project_root = current_path\n",
        "\n",
        "while not os.path.exists(os.path.join(project_root, 'data')) and project_root != '/':\n",
        "    project_root = os.path.dirname(project_root)\n",
        "\n",
        "if project_root == '/':\n",
        "    project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
        "    \n",
        "sys.path.append(project_root)\n",
        "load_dotenv(os.path.join(project_root, \".env\"))\n",
        "\n",
        "print(f\"ğŸ“‚ Project Root: {project_root}\")\n",
        "\n",
        "# ëª¨ë“ˆ ì„í¬íŠ¸\n",
        "from src.retrieval.hybrid_search import HybridRetriever\n",
        "from src.core.config import ZipsaConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš™ï¸ ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” (Initializer)\n",
        "ê²€ìƒ‰í•  **ë²„ì „(Version)**ê³¼ **ì»¬ë ‰ì…˜(Collection)**ì„ ì„ íƒí•˜ì„¸ìš”.\n",
        "- **v2**: Pro ë²„ì „ (ì „ë¬¸ê°€ í•„í„°, ë©€í‹° ë¼ë²¨)\n",
        "- **v3**: ìµœì‹  Clean ë²„ì „ (3-Stage Pipeline, Pickle ì„ë² ë”©)\n",
        "- **ì»¬ë ‰ì…˜**: `care_guides` (ê°€ì´ë“œ), `breeds` (í’ˆì¢…)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# âœ… ì„¤ì • ë³€ê²½\n",
        "VERSION = \"v3\"  # v2, v3\n",
        "COLLECTION = \"care_guides\" # care_guides, breeds\n",
        "\n",
        "print(f\"ğŸš€ Initializing HybridRetriever ({VERSION}, {COLLECTION})...\")\n",
        "retriever = HybridRetriever(version=VERSION, collection_name=COLLECTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§ª 1. ë‹¨ì¼ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (Ad-hoc Test)\n",
        "ì›í•˜ëŠ” ì§ˆë¬¸ì„ ì…ë ¥í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "**Specialist** í•„í„°ë§ì„ ì ìš©í•˜ë©´ íŠ¹ì • ì „ë¬¸ê°€ì˜ í•„í„°ë§ëœ ë¬¸ì„œë§Œ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# âœ… ê²€ìƒ‰ ì¿¼ë¦¬ ì„¤ì •\n",
        "QUERY = \"ì´ˆë³´ ì§‘ì‚¬ê°€ í‚¤ìš°ê¸° ì¢‹ì€ ê³ ì–‘ì´\" \n",
        "# QUERY = \"ê³ ì–‘ì´ê°€ ë°¥ì„ ì•ˆ ë¨¹ì–´\"\n",
        "\n",
        "# âœ… ì „ë¬¸ê°€ í•„í„° (Noneì´ë©´ ì „ì²´ ê²€ìƒ‰)\n",
        "# Options: \"Matchmaker\", \"Physician\", \"Peacekeeper\"\n",
        "SPECIALIST = None \n",
        "# SPECIALIST = \"Physician\" \n",
        "\n",
        "LIMIT = 3\n",
        "\n",
        "# ê²€ìƒ‰ ì‹¤í–‰\n",
        "results = await retriever.search(query=QUERY, specialist=SPECIALIST, limit=LIMIT)\n",
        "\n",
        "print(f\"ğŸ” Query: '{QUERY}' (Filter: {SPECIALIST})\")\n",
        "print(f\"ğŸ“Š Found {len(results)} results:\\n\")\n",
        "\n",
        "for i, doc in enumerate(results, 1):\n",
        "    score = doc.get('final_score', 0.0)\n",
        "    title = doc.get('title_refined', doc.get('title', 'No Title'))\n",
        "    categories = doc.get('categories', [])\n",
        "    specs = doc.get('specialists', [])\n",
        "    \n",
        "    # í…ìŠ¤íŠ¸ ì¼ë¶€ë§Œ ì¶œë ¥\n",
        "    text_preview = doc.get('text', doc.get('content', ''))[:100].replace(\"\\n\", \" \") + \"...\"\n",
        "    \n",
        "    print(f\"{i}. [{score:.4f}] {title}\")\n",
        "    print(f\"   ğŸ·ï¸  Tags: {categories} | ğŸ‘¤ {specs}\")\n",
        "    print(f\"   ğŸ“„ Preview: {text_preview}\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "842489fe",
      "metadata": {},
      "source": [
        "## ğŸ“Š 2. ì„±ëŠ¥ í‰ê°€ (Evaluation)\n",
        "\n",
        "`data/v3/golden_dataset.json` íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ **Hit@K**ì™€ **MRR**ì„ ì¸¡ì •í•©ë‹ˆë‹¤.\n",
        "ì „ì²´ 1,000+ê°œì˜ ë°ì´í„°ì…‹ì„ ëª¨ë‘ í…ŒìŠ¤íŠ¸í•˜ë©´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ `SAMPLE_SIZE`ë¥¼ ì¡°ì ˆí•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d9b598d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# âœ… ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "dataset_path = os.path.join(project_root, \"data/v3/golden_dataset.json\")\n",
        "\n",
        "if os.path.exists(dataset_path):\n",
        "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        FULL_DATASET = json.load(f)\n",
        "    print(f\"ğŸ“š Loaded {len(FULL_DATASET)} test cases from {dataset_path}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ Dataset not found at {dataset_path}. Using mock data.\")\n",
        "    FULL_DATASET = [] # Mock data if needed\n",
        "\n",
        "# âœ… ìƒ˜í”Œë§ (ì „ì²´ë¥¼ ë‹¤ ëŒë¦¬ë ¤ë©´ len(FULL_DATASET)ìœ¼ë¡œ ì„¤ì •)\n",
        "SAMPLE_SIZE = 50  # í…ŒìŠ¤íŠ¸í•  ê°œìˆ˜\n",
        "TEST_DATASET = random.sample(FULL_DATASET, min(SAMPLE_SIZE, len(FULL_DATASET)))\n",
        "\n",
        "print(f\"ğŸ§ª Selected {len(TEST_DATASET)} samples for evaluation.\")\n",
        "\n",
        "# ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜\n",
        "def calculate_metrics(results, expected_keyword):\n",
        "    \"\"\"\n",
        "    ë‹¨ì¼ ì¿¼ë¦¬ ê²°ê³¼ì— ëŒ€í•œ Hit ì—¬ë¶€ì™€ Rankë¥¼ ë°˜í™˜\n",
        "    \"\"\"\n",
        "    for rank, doc in enumerate(results, 1):\n",
        "        # ì œëª©ì´ë‚˜ ë³¸ë¬¸ì— ì •ë‹µ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
        "        # V3 í•„ë“œ ëŒ€ì‘ (title_refined, text) + V2 Fallback\n",
        "        title = doc.get('title_refined', doc.get('title', ''))\n",
        "        text = doc.get('text', doc.get('content', ''))\n",
        "        summary = doc.get('summary', '')\n",
        "        \n",
        "        content = (title + \" \" + text + \" \" + summary).lower()\n",
        "        \n",
        "        if expected_keyword.lower() in content:\n",
        "            return 1, 1/rank  # Hit, Reciprocal Rank\n",
        "    return 0, 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd907e0c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "async def run_evaluation(test_set, k=5):\n",
        "    print(f\"ğŸš€ Starting Evaluation on {len(test_set)} queries (Limit K={k})...\\n\")\n",
        "    \n",
        "    total_hits = 0\n",
        "    total_mrr = 0\n",
        "    results_log = []\n",
        "    \n",
        "    for i, case in enumerate(test_set, 1):\n",
        "        query = case.get(\"query\")\n",
        "        # 'expected_keyword' key used in json, fallback to 'expected' just in case\n",
        "        expected = case.get(\"expected_keyword\", case.get(\"expected\")) \n",
        "        specialist = case.get(\"specialist\")\n",
        "        \n",
        "        # ì§„í–‰ìƒí™© í‘œì‹œ (10ê°œ ë‹¨ìœ„)\n",
        "        if i % 10 == 0: print(f\"...Processing {i}/{len(test_set)}\")\n",
        "        \n",
        "        # ê²€ìƒ‰ ì‹¤í–‰\n",
        "        start_time = time.time()\n",
        "        # Specialist None ì²˜ë¦¬ (ë°ì´í„°ì…‹ì— Noneìœ¼ë¡œ ì €ì¥ëœ ê²½ìš°)\n",
        "        if specialist == \"None\": specialist = None\n",
        "            \n",
        "        docs = await retriever.search(query=query, specialist=specialist, limit=k)\n",
        "        elapsed = time.time() - start_time\n",
        "        \n",
        "        # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
        "        is_hit, rr = calculate_metrics(docs, expected)\n",
        "        total_hits += is_hit\n",
        "        total_mrr += rr\n",
        "        \n",
        "        # ë¡œê·¸ ì €ì¥\n",
        "        top_doc_title = \"-\"\n",
        "        if docs:\n",
        "            top_doc_title = docs[0].get('title_refined', docs[0].get('title', 'No Result'))\n",
        "            \n",
        "        results_log.append({\n",
        "            \"Query\": query,\n",
        "            \"Target\": expected,\n",
        "            \"Filter\": specialist,\n",
        "            \"Hit\": \"âœ…\" if is_hit else \"âŒ\",\n",
        "            \"Rank\": int(1/rr) if rr > 0 else \"-\",\n",
        "            \"Latency\": f\"{elapsed:.3f}s\",\n",
        "            \"Top1_Title\": top_doc_title\n",
        "        })\n",
        "    \n",
        "    # ê²°ê³¼ ì¶œë ¥\n",
        "    df = pd.DataFrame(results_log)\n",
        "    \n",
        "    hit_rate = (total_hits / len(test_set)) * 100\n",
        "    mrr_score = total_mrr / len(test_set)\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(f\"ğŸ† Evaluation Results (K={k})\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"ğŸ¯ Hit@{k}   : {hit_rate:.1f}%\")\n",
        "    print(f\"ğŸ… MRR      : {mrr_score:.4f}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# í‰ê°€ ì‹¤í–‰\n",
        "df_results = await run_evaluation(TEST_DATASET, k=5)\n",
        "df_results.head(20)  # ìƒìœ„ 20ê°œ ê²°ê³¼ í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b033d8e",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "skn-third-proj",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
