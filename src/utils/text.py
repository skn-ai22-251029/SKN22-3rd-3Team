import os
import json
import ssl
from kiwipiepy import Kiwi

# Kiwi ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•œ SSL ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì • (ê¸°ì—…/Mac ë„¤íŠ¸ì›Œí¬ í™˜ê²½ ëŒ€ì‘)
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_kiwi = None
_stopwords = set()
_synonyms = {}

# í•µì‹¬ ë¦¬ì†ŒìŠ¤ ê²½ë¡œ ì°¾ê¸° í—¬í¼
def _get_core_resource_path(filename: str) -> str:
    """ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ì— ìƒê´€ì—†ì´ src/core/tokenizer/ì—ì„œ ë¦¬ì†ŒìŠ¤ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    # í˜„ì¬ íŒŒì¼(src/utils/text.py) ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ ì‹œë„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    prospective_path = os.path.join(current_dir, "../core/tokenizer", filename)
    if os.path.exists(prospective_path):
        return prospective_path
    
    # ì‘ì—… ê³µê°„ ì ˆëŒ€ ê²½ë¡œë¡œ í´ë°±
    return os.path.join("/Users/leemdo/Workspaces/SKN22-3rd-3Team/src/core/tokenizer", filename)

def get_kiwi():
    global _kiwi
    if _kiwi is None:
        _kiwi = Kiwi()
        # ì‚¬ìš©ì ì‚¬ì „ ë¡œë“œ
        path = _get_core_resource_path("domain_dictionary.txt")
        if os.path.exists(path):
            print(f"ğŸ“š ì‚¬ìš©ì ì‚¬ì „ ë¡œë“œ ì¤‘: {path}")
            _kiwi.load_user_dictionary(path)
        else:
            print(f"âš ï¸ ê²½ê³ : ì‚¬ìš©ì ì‚¬ì „ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
    return _kiwi

def load_resources():
    """ë¶ˆìš©ì–´, ìœ ì˜ì–´ ë“± ì™¸ë¶€ ë¦¬ì†ŒìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤ (ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°ë§Œ)."""
    global _stopwords, _synonyms
    
    if not _stopwords:
        path = _get_core_resource_path("stopwords.txt")
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                _stopwords = set(line.strip() for line in f if line.strip())
            print(f"ğŸ›‘ {len(_stopwords)}ê°œì˜ ë¶ˆìš©ì–´ë¥¼ {path}ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"âš ï¸ ê²½ê³ : ë¶ˆìš©ì–´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
    
    if not _synonyms:
        path = _get_core_resource_path("synonyms.json")
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                grouped_data = json.load(f)
                
            # ê³„ì¸µ êµ¬ì¡°ë¥¼ í”Œë«í•˜ê²Œ ë³€í™˜
            for standard_name, aliases in grouped_data.items():
                if isinstance(aliases, list):
                    for alias in aliases:
                        _synonyms[alias] = standard_name
                elif isinstance(aliases, str):
                    _synonyms[standard_name] = aliases
                        
            print(f"ğŸ”„ {len(_synonyms)}ê°œì˜ ìœ ì˜ì–´ ë§¤í•‘ì„ {path}ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"âš ï¸ ê²½ê³ : ìœ ì˜ì–´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")

# ì´ˆê¸° ë¡œë“œ (ì„ íƒ ì‚¬í•­, ë˜ëŠ” tokenizeì—ì„œ ì§€ì—° ë¡œë“œ)
# load_resources()

def tokenize_korean(text: str) -> str:
    """
    Kiwi(ë„ë©”ì¸ ì‚¬ì „ í¬í•¨)ë¥¼ ì‚¬ìš©í•´ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•©ë‹ˆë‹¤.
    ëª…ì‚¬(NN*), ë™ì‚¬(VV), í˜•ìš©ì‚¬(VA), ì–´ê·¼(XR)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ê³  ìœ ì˜ì–´ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
    """
    if not text:
        return ""
        
    kiwi = get_kiwi()
    load_resources() # ë¦¬ì†ŒìŠ¤ ë¡œë“œ ë³´ì¥
    
    tokens = kiwi.tokenize(text)
    
    selected_tokens = []
    for t in tokens:
        # íƒœê·¸ í•„í„°ë§: ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ì–´ê·¼
        if t.tag.startswith(('N', 'V', 'XR')):
            # ë¶ˆìš©ì–´ í•„í„°ë§
            if t.form not in _stopwords:
                # ìœ ì˜ì–´ êµì²´
                token_form = t.form
                if token_form in _synonyms:
                    token_form = _synonyms[token_form]
                    
                selected_tokens.append(token_form)
    
    return " ".join(selected_tokens)
