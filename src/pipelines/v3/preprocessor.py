import os
import re
import json
from typing import List, Dict, Any
from tqdm import tqdm
from src.utils.text import tokenize_korean
from src.core.config import ZipsaConfig
from src.pipelines.base import BasePreprocessor

class V3Preprocessor(BasePreprocessor):
    def __init__(self):
        self.policy = ZipsaConfig.get_policy("v3")
        self.classifier = None # ì´ˆê¸°í™” ì‹œ API í‚¤ ìš”êµ¬ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì§€ì—° ë¡œë“œ ì‚¬ìš©
        self.output_path = "data/v3/processed.json"
        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)

    def clean_text(self, text: str) -> str:
        if not text:
            return ""
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', ' ', text)
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    async def _process_batch(self, batch: List[Dict[str, Any]], start_index: int) -> List[Dict[str, Any]]:
        if self.classifier is None:
            from src.pipelines.v3.classifier import V3Classifier
            self.classifier = V3Classifier()

        # 1. LLM ë¶„ë¥˜ ì‹¤í–‰
        metadata_results = await self.classifier.classify_batch(batch)
        
        processed_batch = []
        for i, (raw_item, meta) in enumerate(zip(batch, metadata_results)):
            uid = f"v3_{start_index + i:05d}"
            
            # í…ìŠ¤íŠ¸ í´ë¦¬ë‹
            clean_content = self.clean_text(raw_item.get("text", ""))
            
            # í˜•íƒœì†Œ ë¶„ì„ (ì œëª© + ìš”ì•½ + ë³¸ë¬¸ ê²°í•© í›„ ìˆ˜í–‰)
            full_text_for_tokens = f"{meta['title_refined']} {meta['metadata']['summary']} {clean_content}"
            tokenized = tokenize_korean(full_text_for_tokens)
            
            doc = {
                "uid": uid,
                "title_refined": meta["title_refined"],
                "text": clean_content,
                "summary": meta["metadata"]["summary"],
                "keywords": meta["metadata"]["keywords"],
                "intent_tags": meta["metadata"]["intent_tags"],
                "categories": meta["categories"],
                "specialists": meta["specialists"],
                "tokenized_text": tokenized,
                "source": "bemypet_catlab",
                "original_url": raw_item.get("url", "")
            }
            processed_batch.append(doc)
        return processed_batch

    async def run(self, limit: int = None) -> str:
        print("ğŸš€ V3 ìˆœìˆ˜ ì „ì²˜ë¦¬ ì‹œì‘ (Raw -> LLM)...")
        
        raw_path = "data/raw/bemypet_catlab.json"
        if not os.path.exists(raw_path):
            raise FileNotFoundError(f"ì›ë³¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {raw_path}")

        with open(raw_path, "r", encoding="utf-8") as f:
            raw_items = json.load(f)

        if limit:
            raw_items = raw_items[:limit]

        processed_items = []
        batch_size = 5 # LLM ë¹„ìš© ë° ì†ë„ ì œí•œ ê´€ë¦¬
        
        print(f"ğŸ“Š {len(raw_items)}ê°œì˜ ë¬¸ì„œë¥¼ ì²˜ë¦¬ ì¤‘ (ë°°ì¹˜ í¬ê¸°: {batch_size})...")

        for i in tqdm(range(0, len(raw_items), batch_size), desc="V3 Preprocessing"):
            batch = raw_items[i : i + batch_size]
            processed_batch = await self._process_batch(batch, i)
            processed_items.extend(processed_batch)

        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)
        with open(self.output_path, "w", encoding="utf-8") as f:
            json.dump(processed_items, f, ensure_ascii=False, indent=2)
            
        print(f"âœ¨ {len(processed_items)}ê°œì˜ í•­ëª©ì„ {self.output_path}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        return self.output_path
