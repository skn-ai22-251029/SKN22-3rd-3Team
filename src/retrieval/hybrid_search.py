from langsmith import traceable
from src.core.config import ZipsaConfig
from src.utils.mongodb import MongoDBManager
from src.embeddings.factory import EmbeddingFactory
from src.utils.text import tokenize_korean

class HybridRetriever:
    def __init__(self, version="v2", collection_name=None):
        self.policy = ZipsaConfig.get_policy(version)
        if version == "v1":
            self.db = MongoDBManager.get_v1_db()
        elif version == "v3":
            self.db = MongoDBManager.get_v3_db()
        else:
            self.db = MongoDBManager.get_v2_db()
        
        # ì •ì±…ì— ìˆëŠ” ì»¬ë ‰ì…˜ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš© (ëª…ì‹œì ìœ¼ë¡œ ì£¼ì–´ì§€ì§€ ì•Šì€ ê²½ìš°)
        self.collection_name = collection_name or self.policy.collection_name
        self.collection = self.db[self.collection_name]
        self.embedder = EmbeddingFactory.get_embedder()

    @traceable(name="Hybrid Search")
    async def search(self, query: str, specialist: str = None, limit: int = 3, filters: dict = None):
        """
        RRF (Reciprocal Rank Fusion)ë¥¼ ì‚¬ìš©í•˜ì—¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        print(f"ğŸ” [RETRIEVER]: '{query}' ê²€ìƒ‰ ì¤‘ (ì „ë¬¸ê°€: {specialist}, í•„í„°: {filters})...")
        
        # 1. ì…ë ¥ ì „ì²˜ë¦¬
        if specialist == "General":
            specialist = None
            
        # 2. ê°œë³„ ê²€ìƒ‰ ì‹¤í–‰ (ë²¡í„° ë° í‚¤ì›Œë“œ)
        vector_results = await self._run_vector_search(query, specialist, filters, limit)
        keyword_results = await self._run_keyword_search(query, specialist, filters, limit)

        # 3. RRFë¥¼ ì‚¬ìš©í•œ ê²°í•© ë° ë­í‚¹
        merged = self._rank_and_merge(vector_results, keyword_results, limit)
        
        print(f"âœ… [RETRIEVER]: {len(merged)}ê±´ì˜ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        return merged

    async def _run_vector_search(self, query: str, specialist: str, filters: dict, limit: int):
        """Atlas ë²¡í„° ê²€ìƒ‰ ë¡œì§ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        query_vector = await self.embedder.embed_query(query)
        
        # ë©”íƒ€ë°ì´í„° í•„í„° êµ¬ì„±
        combined_filter = {}
        if specialist:
            combined_filter["specialists"] = specialist
        if filters:
            combined_filter.update(filters)
            
        vector_search_stage = {
            "$vectorSearch": {
                "index": "vector_index",
                "path": "embedding",
                "queryVector": query_vector,
                "numCandidates": 100,
                "limit": limit * 2
            }
        }
        if combined_filter:
            vector_search_stage["$vectorSearch"]["filter"] = combined_filter

        try:
            return await self.collection.aggregate([
                vector_search_stage,
                { "$set": { "score_type": "vector" } }
            ]).to_list(None)
        except Exception as e:
            print(f"ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    async def _run_keyword_search(self, query: str, specialist: str, filters: dict, limit: int):
        """ë©”íƒ€ë°ì´í„° í•„í„°ë§ì„ í¬í•¨í•œ Atlas ê²€ìƒ‰ (BM25)ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        try:
            tokenized_query = tokenize_korean(query)
            
            if specialist or filters:
                must_clauses = [{"text": {"query": tokenized_query, "path": "tokenized_text"}}]
                filter_clauses = []
                
                if specialist:
                    filter_clauses.append({"text": {"query": specialist, "path": "specialists"}})
                
                if filters:
                    for k, v in filters.items():
                        if isinstance(v, dict):
                            if "$lte" in v: filter_clauses.append({"range": {"path": k, "lte": v["$lte"]}})
                            elif "$gte" in v: filter_clauses.append({"range": {"path": k, "gte": v["$gte"]}})
                            elif "$eq" in v: filter_clauses.append({"equals": {"path": k, "value": v["$eq"]}})
                        else:
                            filter_clauses.append({"equals": {"path": k, "value": v}})

                search_query = {
                    "index": "keyword_index",
                    "compound": {
                        "must": must_clauses,
                        "filter": filter_clauses if filter_clauses else [{"wildcard": {"path": "*", "query": "*"}}]
                    }
                }
            else:
                search_query = {
                    "index": "keyword_index",
                    "text": {"query": tokenized_query, "path": "tokenized_text"}
                }

            return await self.collection.aggregate([
                { "$search": search_query },
                { "$limit": limit * 2 },
                { "$set": { "score_type": "keyword" } }
            ]).to_list(None)
        except Exception as e:
            print(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def _rank_and_merge(self, vector_results, keyword_results, limit):
        """RRF (Reciprocal Rank Fusion)ë¥¼ ì ìš©í•˜ê³  ê²°ê³¼ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤."""
        scores = {}
        for rank, doc in enumerate(vector_results):
            doc_id = str(doc.get("_id"))
            scores[doc_id] = scores.get(doc_id, 0) + 1 / (rank + 60)

        for rank, doc in enumerate(keyword_results):
            doc_id = str(doc.get("_id"))
            scores[doc_id] = scores.get(doc_id, 0) + 1 / (rank + 60)
        
        all_docs = vector_results + keyword_results
        if not all_docs:
            return []

        all_docs_sorted = sorted(all_docs, key=lambda x: scores.get(str(x.get("_id")), 0), reverse=True)
        
        merged = []
        seen = set()
        for doc in all_docs_sorted:
            doc_id = str(doc.get("_id"))
            if doc_id not in seen:
                doc["final_score"] = scores[doc_id]
                merged.append(doc)
                seen.add(doc_id)
        
        return merged[:limit]

# ì‚¬ìš© ì˜ˆì‹œ
# retriever = HybridRetriever(collection_name="breeds")
# results = await retriever.search("í™œë™ì ì¸ ê³ ì–‘ì´ ì¶”ì²œí•´ì¤˜")
